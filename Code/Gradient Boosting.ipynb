{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Making an predictive model using Gradient Boosting Machine</h1>\n",
    "In deze jupyter notebook file, worden de de decision tree gemaakt. \n",
    "Dit wordt gedaan om de onderzoeksvraag van mijn thesis te kunnen beantwoorden:\n",
    "To what extent can support vector machine, randomforest tree, or Gradient Boosting Machine contributeto predicting the demand for the specialist youth caresegments in Amsterdam?\n",
    "Ook is dit nodig voor het beantwoorden van mijn sub vraen:\n",
    "•Are there neighborhood socio-demographic characteristics which are predictive of the use of youth caresegments?\n",
    "•Which of the tested models has the highest f1 score in predicting the youth care segment use?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hier onder worden eerst de benodigde librabry geimporteerd</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from numpy import mean\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from xgboost import plot_importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de sub onderzoeks vraag: Which of the tested models has the highest f1-score in predicting the youth care segment use? Waarom we voor deze score hebben gekozen, kan gelezen worden onder het kopje \"model eveluation\".\n",
    "\n",
    "Ook maken we een aantal variabele hier aan om de code zo gestructuurd mogelijk te houden. Waarom deze nodig zijn, valt te lezen in het kopje \"model making'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some variables\n",
    "param_grid = {\n",
    "        'max_depth': range(4,26,4),\n",
    "        'scale_pos_weight' : [1, 25, 50, 75, 100],\n",
    "        'colsample_bytree': np.arange(0.5,1.0,0.3),\n",
    "}\n",
    "\n",
    "\n",
    "cv_method = RepeatedStratifiedKFold(n_splits=2, \n",
    "                                    n_repeats=1, \n",
    "                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Laad de data in, die gemaakt is uit de andere jupyter notebook file</h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"C:\\\\VERTROUWELIJK\\\\final_dataSet.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data in indepentend variable an dependent variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Voorziening'], axis=1)\n",
    "X_encoded = pd.get_dummies(X, columns=['Geslacht'])\n",
    "y = df['Voorziening'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode string class values as integers, which is needed for Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(y)\n",
    "label_encoded_y = label_encoder.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make train and test set. Waarom dit nodig is, zie \"making model\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_encoded, label_encoded_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Make the first gradient Boosting</h3>\n",
    "and fit this to get the scores. Dit is nodig om alle onderzoeksvragen mee te bentwoorden.\n",
    "Zie \"model\" in de thesis waarom we voro xgboost hebben gekozen en niet noor de standaard aangeleverde vanuit sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
      "              scale_pos_weight=None, subsample=None, tree_method=None,\n",
      "              validate_parameters=None, verbosity=None)\n",
      "f1_score: 72.67%\n"
     ]
    }
   ],
   "source": [
    "# fit model no training data\n",
    "model = xgboost.XGBClassifier()\n",
    "start_time=time.time()\n",
    "# model.fit(X_train, y_train)\n",
    "print(model)\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_micro')\n",
    "score = mean(scores)\n",
    "print(\"f1_score: %.2f%%\" % (score * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals gezegd, de data is erg imbalanced. Daarom maken we een decision tree with random undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.361\n"
     ]
    }
   ],
   "source": [
    "steps = [('under', RandomUnderSampler()), ('model', xgboost.XGBClassifier())]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# evaluate pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=1, random_state=42)\n",
    "scores = cross_val_score(pipeline, X_encoded, label_encoded_y, scoring='f1_micro', cv=cv, n_jobs=-1, verbose = 10)\n",
    "score = mean(scores)\n",
    "print('F1 Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prune the model</h3>\n",
    "Na het vergelijken van de twee modellen, heeft het model met de normale data set de hoogste F1 score, hier na gaan we het model nog prunen. Zie Making Model in de thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 60 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed: 28.5min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 39.8min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed: 56.8min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed: 69.3min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed: 82.8min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed: 109.3min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed: 141.5min\n",
      "[Parallel(n_jobs=-1)]: Done 118 out of 120 | elapsed: 179.9min remaining:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed: 183.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:57:15] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "11099.227629184723\n"
     ]
    }
   ],
   "source": [
    "# make an gridSearch\n",
    "start_time=time.time()\n",
    "grid = GridSearchCV(xgboost.XGBClassifier(),\n",
    "                   param_grid=param_grid,\n",
    "                    scoring='f1_micro',\n",
    "                   verbose=10,\n",
    "                    cv= cv_method,\n",
    "                   n_jobs=-1)\n",
    "start_time=time.time()\n",
    "\n",
    "grid.fit(X_train,y_train)\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8, 'max_depth': 4, 'scale_pos_weight': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best parameters of coming from the gridSearch.\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6769638107303526"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best F1 score, coming from the gridSearch.\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de onderzoeks vraag: Are there neighborhood socio-demographic characteristics which are predictive of the use of youth caresegments? is onderstaande code nodig. Echter voeren wij deze niet uit, omdat Decicion tree een hogere F1 score heeft. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "need to call fit or load_model beforehand",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-82e2295227c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Plot the top 10 most important features of the XGboost model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mxgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_importance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_num_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\plotting.py\u001b[0m in \u001b[0;36mplot_importance\u001b[1;34m(booster, ax, height, xlim, ylim, title, xlabel, ylabel, fmap, importance_type, max_num_features, grid, show_values, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXGBModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         importance = booster.get_booster().get_score(\n\u001b[0m\u001b[0;32m     64\u001b[0m             importance_type=importance_type, fmap=fmap)\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mget_booster\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_Booster'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'need to call fit or load_model beforehand'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Booster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: need to call fit or load_model beforehand"
     ]
    }
   ],
   "source": [
    "# Plot the top 10 most important features of the XGboost model\n",
    "# xgboost.plot_importance(model, max_num_features=10)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
