{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Making an predictive model using Decision Tree</h1>\n",
    "In deze jupyter notebook file, worden de de decision tree gemaakt. \n",
    "Dit wordt gedaan om de onderzoeksvraag van mijn thesis te kunnen beantwoorden:\n",
    "To what extent can support vector machine, randomforest tree, or Gradient Boosting Machine contributeto predicting the demand for the specialist youth caresegments in Amsterdam?\n",
    "Ook is dit nodig voor het beantwoorden van mijn sub vraen:\n",
    "•Are there neighborhood socio-demographic characteristics which are predictive of the use of youth caresegments?\n",
    "•Which of the tested models has the highest f1 score in predicting the youth care segment use?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hier onder worden eerst de benodigde librabry geimporteerd</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import mean\n",
    "from sklearn import decomposition, datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de sub onderzoeks vraag: Which of the tested models has the highest f1-score in predicting the youth care segment use? Waarom we voor deze score hebben gekozen, kan gelezen worden onder het kopje \"model eveluation\".\n",
    "\n",
    "Ook maken we een aantal variabele hier aan om de code zo gestructuurd mogelijk te houden. Waarom deze nodig zijn, valt te lezen in het kopje \"model making'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Def inorder to calcualte some metrics\n",
    "def calculateMetrics(model):\n",
    "        y_predicted = model.predict(X_test)\n",
    "        print(model)\n",
    "        print (\"precision_score\")\n",
    "        print(precision_score(y_test, y_predicted, average='micro'))\n",
    "        print (\"recall score\")\n",
    "        print(recall_score(y_test, y_predicted, average='micro'))\n",
    "        print (\"F1 score\")\n",
    "        print(f1_score(y_test, y_predicted, average='micro'))\n",
    "        \n",
    "# Some variables\n",
    "param_dict = { \n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': range(4,26,4),\n",
    "    'min_samples_split': range(1,10,2),\n",
    "    'min_samples_leaf': range(1,5)\n",
    "}\n",
    "\n",
    "cv_method = RepeatedStratifiedKFold(n_splits=5, \n",
    "                                    n_repeats=3, \n",
    "                                    random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Laad de data in, die gemaakt is uit de andere jupyter notebook file</h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "df = pd.read_pickle(\"C:\\\\VERTROUWELIJK\\\\final_dataSet.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data in indepentend variable an dependent variable. Also get dummies from the binary values in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Voorziening'], axis=1)\n",
    "X_encoded = pd.get_dummies(X, columns=['Geslacht'])\n",
    "y = df['Voorziening'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make test and train set. Waarom dit nodig is, zie \"making model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded,y,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h4>Make the first DecisionTree</h4>\n",
    " and fit this to get the scores. Dit is nodig om alle onderzoeksvragen mee te bentwoorden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(random_state=42)\n",
      "f1_score: 66.42%\n"
     ]
    }
   ],
   "source": [
    "# Make the first DecisionTree, and fit this to get the scores.\n",
    "clf_dt = DecisionTreeClassifier(random_state = 42)\n",
    "print(clf_dt)\n",
    "scores = cross_val_score(clf_dt, X_train, y_train, cv=5, scoring='f1_micro')\n",
    "score = mean(scores)\n",
    "print(\"f1_score: %.2f%%\" % (score * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals gezegd, de data is erg imbalanced. Daarom maken we een decision tree with random undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('under', RandomUnderSampler()), ('model', DecisionTreeClassifier(random_state=42))]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# evaluate pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=100, n_repeats=5, random_state=42)\n",
    "scores = cross_val_score(pipeline, X_encoded, y, scoring='f1_micro', cv=cv, n_jobs=-1)\n",
    "score = mean(scores)\n",
    "# calculate the mean of all these models. \n",
    "print('F1 Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Prune the model</h4>\n",
    "Na het vergelijken van de twee modellen, heeft het model met de normale data set de hoogste F1 score, hier na gaan we het model nog prunen. Zie Making Model in de thesis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(clf_dt,\n",
    "                   param_grid=param_dict,\n",
    "                   cv=cv_method,\n",
    "                    scoring='f1_micro',\n",
    "                   verbose=1,\n",
    "                   n_jobs=-1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters after doing the gridsearch\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the last DecisionTree with best parameter coming from the GridSearch. \n",
    "clf_dt = DecisionTreeClassifier(max_depth=16,criterion=\n",
    "                            'entropy',min_samples_leaf=1, min_samples_split=3)\n",
    "clf_dt = clf_dt.fit(X_train, y_train)\n",
    "calculateMetrics(clf_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Confusion Matrix to get an insight of how well the model is performing. \n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "test = plot_confusion_matrix(clf_dt, X_test, y_test, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visulation the gridsearch, we needed to make an figure. This can be seen in Figure X in the thesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_DT = pd.DataFrame(grid.cv_results_['params'])\n",
    "results_DT['test_score'] = grid.cv_results_['mean_test_score']\n",
    "for i in ['gini', 'entropy']:\n",
    "    temp = results_DT[results_DT['criterion'] == i]\n",
    "    temp_average = temp.groupby('max_depth').agg({'test_score': 'mean'})\n",
    "    plt.plot(temp_average, marker = '.', label = i)\n",
    "    \n",
    "    \n",
    "plt.legend()\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel(\"Mean CV Score\")\n",
    "plt.title(\"DT Performance Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de onderzoeks vraag: Are there neighborhood socio-demographic characteristics which are predictive of the use of youth caresegments? is onderstaande code nodig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(clf_dt.feature_importances_)\n",
    "feature_list = list(X.columns)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
